# Web Traffic Monitoring Platform

A real-time web application traffic monitoring platform that processes and analyzes web access logs using modern data engineering tools and practices.

## ğŸš€ Project Overview

This project implements a complete data pipeline that:
- Captures web application access events in real-time
- Processes data in both batch and streaming modes
- Stores data in a Data Lake
- Performs analytics with Spark
- Exposes APIs with aggregated insights
- Implements basic ML for anomaly detection

## ğŸ› ï¸ Tech Stack

- **Data Processing**: Python, Pandas, PySpark
- **Stream Processing**: Apache Kafka
- **Storage**: AWS S3, HDFS
- **Analytics**: Apache Spark, AWS Athena
- **API**: FastAPI
- **Infrastructure**: Terraform, AWS
- **Orchestration**: Apache Airflow
- **ML**: scikit-learn
- **CI/CD**: GitHub Actions

## ğŸ“ Project Structure

```
â”œâ”€â”€ data-generator/     # Log simulation scripts
â”œâ”€â”€ kafka-ingest/       # Kafka producer/consumer
â”œâ”€â”€ spark-jobs/         # Spark processing jobs
â”œâ”€â”€ airflow/           # DAGs and Airflow config
â”œâ”€â”€ api/               # FastAPI application
â”œâ”€â”€ terraform/         # Infrastructure as Code
â”œâ”€â”€ scripts/           # Utility scripts
â””â”€â”€ docs/             # Documentation
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Docker and Docker Compose
- AWS CLI (for cloud deployment)
- Terraform
- Apache Spark
- Kafka

### Local Development Setup

1. Clone the repository:
```bash
git clone [repository-url]
cd web-traffic-monitor
```

2. Create and activate virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Start Kafka and other services:
```bash
docker-compose up -d
```

5. Run the data generator:
```bash
python data-generator/main.py
```

## ğŸ“Š Architecture

[Diagram will be added here]

## ğŸ”„ Data Flow

1. Web access logs are generated by the data generator
2. Logs are ingested into Kafka
3. Spark processes the data in both batch and streaming modes
4. Processed data is stored in S3/HDFS
5. FastAPI serves the processed data and insights
6. Airflow orchestrates the entire pipeline

## ğŸ“ˆ Features

- Real-time log ingestion
- Batch and streaming processing
- Anomaly detection
- REST API for insights
- Infrastructure as Code
- CI/CD pipeline

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.